[id="ocp-4-7-release-notes"]
= {product-title} {product-version} release notes
include::modules/common-attributes.adoc[]
:context: release-notes

toc::[]

Red Hat {product-title} provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management overhead. {product-title} supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.

Built on {op-system-base-full} and Kubernetes, {product-title} provides a more secure and scalable multi-tenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. {product-title} enables organizations to meet security, privacy, compliance, and governance requirements.

[id="ocp-4-7-about-this-release"]
== About this release

// TODO: Update k8s link once there is a version-specific URL for the Kubernetes release
(link:https://access.redhat.com/errata/RHBA-2020:1234[RHBA-2020:1234]) is now available. This release uses link:https://kubernetes.io/docs/setup/release/notes/[Kubernetes 1.20] with CRI-O runtime. New features, changes, and known issues that pertain to {product-title} {product-version} are included in this topic.

//Red Hat did not publicly release {product-title} 4.6.0 as the GA version and, instead, is releasing {product-title} 4.6.1 as the GA version.

{product-title} {product-version} clusters are available at https://cloud.redhat.com/openshift. The {cloud-redhat-com} application for {product-title} allows you to deploy OpenShift clusters to either on-premise or cloud environments.

{product-title} {product-version} is supported on {op-system-base-full} 7.7 or later, as well as {op-system-first} 4.6.

You must use {op-system} machines for the control plane, which are also known as master machines, and you can use either {op-system} or {op-system-base-full} 7.7 or later for compute machines, which are also known as worker machines.

[IMPORTANT]
====
Because only {op-system-base-full} version 7.7 or later is supported for compute machines, you must not upgrade the {op-system-base} compute machines to version 8.
====

//{product-title} 4.6 is an Extended Update Support (EUS) release. More information on Red Hat OpenShift EUS is available in link:https://access.redhat.com/support/policy/updates/openshift#ocp4_phases[OpenShift Life Cycle] and link:https://access.redhat.com/support/policy/updates/openshift-eus[OpenShift EUS Overview].

With the release of {product-title} 4.7, version 4.4 is now end of life. For more information, see the link:https://access.redhat.com/support/policy/updates/openshift[Red Hat OpenShift Container Platform Life Cycle Policy].

[id="ocp-4-7-inclusive-language"]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[Red Hat CTO Chris Wrightâ€™s message].

[id="ocp-4-7-new-features-and-enhancements"]
== New features and enhancements

This release adds improvements related to the following components and concepts.

[id="ocp-4-7-rhcos"]
=== {op-system-first}

[id="ocp-4-7-enhanced-disk-provisioning"]
==== Enhanced disk provisioning for LUKS, RAID, and FBA DASD

{product-title} 4.7 includes several improvements to disk provisioning for bare metal deployments. The following features are currently supported for new 4.7 clusters only:

* Native Ignition support for LUKS disk encryption provides additional configurability for encrypted root filesystems, as well as support for encryption of additional data filesystems.
* {op-system} now supports boot disk mirroring, except on s390x, providing redundancy in the case of disk failure.
* {op-system} on s390x can be installed onto fixed-block architecture (FBA)-type direct access storage device (DASD) disks.

* Multipathing is now supported during initial deployment and early boot for storage devices when attached to clusters that are created using {product-title} 4.7 or higher.

[NOTE]
====
On new clusters, LUKS configuration must use the native Ignition mechanism, as provisioning fails if the legacy `/etc/clevis.json` file is included in the machine config. On clusters that are upgrading from {product-title} 4.6 or earlier, LUKS can only be configured by using `/etc/clevis.json`.
====

[id="ocp-4-7-bootupd"]
==== Update the bootloader by using `bootupd`

With `bootupd`, {op-system} users now have access to a cross-distribution, system-agnostic OS update tool that manages firmware and boot updates in UEFI and legacy BIOS boot modes that run on modern architectures.

[id="ocp-4-7-rhcos-rhel-8-3-packages"]
==== {op-system} now supports RHEL 8.3

{op-system} is now using Red Hat Enterprise Linux (RHEL) 8.3 packages. {product-title} 4.6 and below will stay with RHEL 8.2 packages. This enables you to have the latest fixes, features, and enhancements, such as NetworkManager features, as well as the latest hardware support and driver updates.

[id="ocp-4-7-rhcos-kdump"]
==== {op-system} now supports `kdump` service (Technical Preview)
The `kdump` service is introduced in Technical Preview in {op-system} to provide a crash-dumping mechanism for debugging kernel issues. You can use this service to save system memory content for later analysis. The `kdump` service is not managed at the cluster-level and must be enabled and configured manually on a per-node basis.

[id="ocp-4-7-ignition"]
==== Ignition updates
The following Ignition updates are now available:

* {op-system} now supports Ignition config spec 3.2.0. This update provides support for disk partition resizing, LUKS encrypted storage, and `gs://` URLs.
* When executing in non-default AWS partitions, such as GovCloud or AWS China, Ignition now fetches `s3://` resources from the same partition.
* Ignition now supports AWS EC2 Instance Metadata Service Version 2 (IMDSv2).

[id="ocp-4-7-installation-and-upgrade"]
=== Installation and upgrade

[id="ocp-4-7-aws-c2s-secret-region"]
==== Installing a cluster into the AWS C2S Secret Region

You can now install a cluster on Amazon Web Services (AWS) into the Commercial Cloud Services (C2S) Secret Region. Because the C2S region does not have an {op-system} AMI published by Red Hat, you must upload a custom AMI that belongs to that region. You are also required to include the CA certificates for C2S in the `additionalTrustBundle` field of the `install-config.yaml` file during cluster installation. Clusters deployed to the C2S Secret Region do not have access to the Internet; therefore, you must configure a private image registry.

The installation program does not support destroying a cluster deployed to the C2S region; you must manually remove the resources of the cluster.

//Link for more info when available.

[id="ocp-4-7-gcp-disk-encryption"]
==== Installing cluster on GCP with disk encryption using a personal encryption key

You can now install a cluster on Google Cloud Platform (GCP) and use a personal encryption key to encrypt both virtual machines and persistent volumes. This is done by setting the `controlPlane.platform.gcp.osDisk.encryptionKey`, `compute.platform.gcp.osDisk.encryptionKey`, or `gcp.defaultMachinePlatform.osDisk.encryptionKey` field in the `install-config.yaml` file.

//Link for more info when available.

[id="ocp-4-7-web-console"]
=== Web console

[id="ocp-4-7-web-console-localization"]
==== Web console localization

The web console is now localized and provides language support for global users. English, Japanese, Simplified Chinese, and Korean are currently supported. The displayed language follows your browser preferences, but you can also select a language to override the browser default. From the *Admin* drop-down menu, select *Language preferences* to update your language setting. Localized date and time is now also supported.

[id="ocp-4-7-web-console-insights-plugin"]
==== Insights plug-in

The xref:../support/remote_health_monitoring/using-insights-to-identify-issues-with-your-cluster.adoc#displaying-the-insights-status-in-the-web-console_using-insights-to-identify-issues-with-your-cluster[Insights plug-in] is now integrated into the {product-title} web console. Insights provides cluster health data, such as the number of total issues and total risks of the issues. Risks are labeled as *Critical*, *Important*, *Moderate*, or *Low*. You can quickly navigate to {cloud-redhat-com} for further details about the issues and how to fix them.

[id="ocp-4-7-security"]
=== Security and compliance

[id="ocp-4-7-security-user-oauth-tokens"]
==== Managing user-owned OAuth access tokens

Users can now manage their own OAuth access tokens. This allows users to review their tokens and delete any tokens that have timed out or are no longer needed.

For more information, see xref:../authentication/managing-oauth-access-tokens.adoc#managing-oauth-access-tokens[Managing user-owned OAuth access tokens].

[id="ocp-4-7-security-cco-deletion-gcp-creds-mint-mode"]
==== Cloud Credential Operator support for deletion of GCP root credentials after installation

You can now remove or rotate the GCP admin-level credential that the xref:../operators/operator-reference.adoc#cloud-credential-operator_red-hat-operators[Cloud Credential Operator] uses in Mint mode. This option requires the presence of the admin-level credential during installation, but the credential is not stored in the cluster permanently and does not need to be long-lived.

[id="ocp-4-7-compliance-cis-benchmark"]
==== CIS Kubernetes Benchmark profile for the Compliance Operator

You can now use the Compliance Operator to perform Center for Internet Security (CIS) Kubernetes Benchmark checks. CIS profiles for {product-title} are based on the CIS Kubernetes checks.

[id="ocp-4-7-security-secure-boot"]
==== Secure Boot support for installer-provisioned clusters

You can now deploy a cluster with Secure Boot when using installer-provisioned infrastructure on bare metal nodes. Deploying a cluster with Secure Boot requires UEFI boot mode and Red Fish Virtual Media. You cannot use self-generated keys with Secure Boot.

[id="ocp-4-7-networking"]
=== Networking

[id="ocp-4-7-ovn-kubernetes-migration"]
==== Expanded platform support for migrating from the OpenShift SDN cluster network provider to the OVN-Kubernetes cluster network provider

A xref:../networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc#migrate-from-openshift-sdn[migration to the OVN-Kubernetes cluster network provider] is now supported on installer-provisioned clusters on the following platforms:

* Bare metal hardware
* Amazon Web Services (AWS)
* Google Cloud Platform (GCP)
* Microsoft Azure
* {rh-openstack-first}
* VMware vSphere

[id="ocp-4-7-ovn-kubernetes-egress-firewall-dns"]
==== OVN-Kubernetes egress firewall support for DNS rules

When configuring an egress firewall rule, you can now use a xref:../networking/ovn_kubernetes_network_provider/configuring-egress-firewall-ovn.adoc#domain-name-server-resolution_configuring-egress-firewall-ovn[DNS domain name] instead of an IP address.
With the addition of DNS support in the OVN-Kubernetes cluster network provider egress firewall implementation, parity is achieved with the OpenShift SDN cluster network provider egress firewall implementation.

[id="ocp-4-7-storage"]
=== Storage

[id="ocp-4-7-storage-csi-snapshots"]
==== Persistent storage using CSI volume snapshots is generally available

You can use the Container Storage Interface (CSI) to create, restore, and delete a volume snapshot when using CSI drivers that provide support for volume snapshots. This feature was previously introduced as a Technology Preview feature in {product-title} 4.4 and is now generally available and enabled by default in {product-title} 4.7.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-snapshots.adoc#persistent-storage-csi-snapshots[Using CSI volume snapshots].

[id="ocp-4-5-persistent-storage-csi-gcp-pd"]
==== Persistent storage using the GCP PD CSI Driver Operator (Technology Preview)

The Google Cloud Platform (GCP) persistent disk (PD) CSI driver is automatically deployed and managed on GCP environments, allowing you to dynamically provision these volumes without having to install the driver manually. The GCP PD CSI Driver Operator that manages this driver is in Technology Preview.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-gcp-pd.adoc#persistent-storage-csi-gcp-pd[GCP PD CSI Driver Operator].

[id="ocp-4-5-persistent-storage-csi-cinder"]
==== Persistent storage using the OpenStack Cinder CSI Driver Operator

You can now use CSI to provision a persistent volume using the CSI driver for OpenStack Cinder.

For more information, see xref:../storage/container_storage_interface/persistent-storage-csi-cinder.adoc#persistent-storage-csi-cinder[OpenStack Cinder CSI Driver Operator].

[id="ocp-4-7-storage-vsphere-problem-detector"]
==== vSphere Problem Detector Operator

The vSphere Problem Detector Operator periodically checks functionality of {product-title} clusters installed in a vSphere environment. The vSphere Problem Detector Operator is installed by default by the Cluster Storage Operator, allowing you to quickly identify and troubleshoot common storage issues, such as configuration and permissions, on vSphere clusters.

[id="ocp-4-7-registry"]
=== Registry

[id="ocp-4-7-registry-oci-support"]
==== Open Container Initiative images support

The {product-title} internal registry and image streams now support Open Container Initiative (OCI) images. You can use OCI images in the same way you would use Docker `schema2` images.

//Add link

[id="ocp-4-7-olm"]
=== Operator lifecycle

[id="ocp-4-7-safe-operator-upgrades"]
==== Safe Operator upgrades

To make upgrades more robust, it is recommend that Operators actively communicate with the service that is about to be updated. If a service is processing a critical operation, such as live migrating virtual machines (VMs) in OpenShift Virtualization or restoring a database, it might be unsafe to upgrade the related Operator at that time.

In {product-title} 4.7, Operators can take advantage of the new `OperatorCondition` resource to communicate a non-upgradeable state to Operator Lifecycle Manager (OLM), such as when a related service is performing a critical operation. The non-upgradeable state delays any pending Operator upgrade, whether automatically or manually approved, until the Operator finishes the operation and reports upgrade readiness.

See _Operator conditions_ for more about how OLM uses this communication channel.

// Will update to an xref after related PR merge

See _Managing Operator conditions_ for details on overriding states in OLM as a cluster administrator.

// Will update to an xref after related PR merge

See xref:../operators/operator_sdk/osdk-generating-csvs.adoc#osdk-operatorconditions_osdk-generating-csvs[Enabling Operator conditions] for details on updating your project as an Operator developer to use the communication channel.

[id="ocp-4-7-pull-secrets-cs"]
==== Adding pull secrets to catalog sources

If certain images relevant to Operators managed by Operator Lifecycle Manager (OLM) are hosted in an authenticated container image registry, also known as a private registry, OLM and OperatorHub are unable to pull the images by default. To enable access, you can create a pull secret that contains the authentication credentials for the registry.

By referencing one or more secrets in a catalog source, some of these required images can be pulled for use in OperatorHub, while other images require updates to the global cluster pull secret.

See _Accessing images for Operators from private registries_ for more details.

// Will update to an xref after related PR merge

[id="ocp-4-7-osdk"]
=== Operator development

[id="ocp-4-7-osdk-supported"]
==== Operator SDK now fully supported

As of {product-title} 4.7, the Operator SDK is now a fully supported Red Hat offering. With the downstream release of Operator SDK v1.3.0, officially supported and branded Operator SDK tooling is now available for download directly from Red Hat.

The Operator SDK CLI assists Operator developers and independent software vendor (ISV) partners in writing Operators that provide a great user experience and are compatible with OpenShift distributions and Operator Lifecycle Manager (OLM).

The Operator SDK enables Operator authors with cluster administrator access to a Kubernetes-based cluster, such as {product-title}, to develop their own Operators based on Go, Ansible, or Helm. For Go-based Operators, link:https://kubebuilder.io/[Kubebuilder] is embedded into the SDK as the scaffolding solution; this means existing Kubebuilder projects can be used as is with the SDK and continue to work.

The following features highlight some of the capabilities of the Operator SDK:

Native support for Operator Bundle Format:: The Operator SDK includes native support for the Operator Bundle Format introduced in {product-title} 4.6. All metadata required to package an Operator for OLM is generated automatically. This support enables Operator developers to package and test their Operator for OLM and OpenShift distributions directly from their CI pipelines.

Operator Lifecycle Manager integration:: The Operator SDK provides developers with a streamlined experience for quickly testing their Operator with OLM from their workstation. The `operator-sdk run` command runs an Operator on a cluster to test whether it behaves correctly when managed by OLM.

Webhook integration:: The Operator SDK supports webhook integration with OLM, which simplifies installing Operators that have admission or custom resource definition (CRD) conversion webhooks. This feature relieves the cluster administrator of having to manually register the webhooks, add TLS certificates, and set up certificate rotation.

Validation scorecard:: Operator authors should validate that their Operator is packaged correctly and free of syntax errors. To validate an Operator, the scorecard tool provided by the Operator SDK begins by creating all resources required by any related custom resources (CRs) and the Operator. The scorecard then creates a proxy container in the deployment of the Operator, which is used to record calls to the API server and run some of the tests. The tests performed also examine some of the parameters in the CRs.

Upgrade readiness reporting:: Operator developers can use the Operator SDK to take advantage of code scaffolding support for Operator conditions, including reporting upgrade readiness to OLM.

See xref:../operators/operator_sdk/osdk-getting-started.adoc#osdk-getting-started[Developing Operators] for full documentation on the Operator SDK.

[id="ocp-4-7-images"]
=== Images


[id="ocp-4-7-machine-api"]
=== Machine API

[id="ocp-4-7-machine-api-wait-upgrade"]
==== Updates are immediately blocked if a machine config pool is degraded

If a machine config pool (MCP) is in a `degraded` state, the Machine Config Operator (MCO) now reports its *Upgradeable* status as *False*. As a result, you are now prevented from performing an update within a minor version, for example, from 4.7 to 4.8, until all machine config pools are healthy. Previously, with a degraded machine config pool, the Machine Config Operator did not report its *Upgradeable* status as *false*. The update was allowed and would eventually fail when updating the Machine Config Operator because of the degraded machine config pool. There is no change in this behavior for updates within z-stream releases, for example, from 4.7.1 to 4.7.2. As such, you should check the machine config pool status before performing a z-stream update.

[id="ocp-4-7-nodes"]
=== Nodes

[id="ocp-4-7-nodes-descheduler-ga"]
==== Descheduler is generally available

The descheduler is now generally available. The descheduler provides the ability to evict a running pod so that the pod can be rescheduled onto a more suitable node. You can enable one or more of the following descheduler profiles:

* `AffinityAndTaints`: evicts pods that violate inter-pod anti-affinity, node affinity, and node taints.
* `TopologyAndDuplicates`: evicts pods in an effort to evenly spread similar pods, or pods of the same topology domain, among nodes.
* `LifecycleAndUtilization`: evicts long-running pods and balances resource usage between nodes.

[NOTE]
====
With the GA, you can enable descheduler profiles and configure the descheduler interval. Any other settings that were available during Technology Preview are no longer available.
====

For more information, see xref:../nodes/scheduling/nodes-descheduler.adoc#nodes-descheduler[Evicting pods using the descheduler].

[id="ocp-4-7-nodes-autoscaling-ga"]
==== Autoscaling for memory utilization GA

Autoscaling for memory utilization is now generally available. You can create horizontal pod autoscaler custom resources to automatically scale the pods associated with a deployment config or replication controller to maintain the average memory utilization you specify, either a direct value or a percentage of requested memory. For more information, see xref:../nodes/pods/nodes-pods-autoscaling.html#nodes-pods-autoscaling-creating-memory_nodes-pods-autoscaling[Creating a horizontal pod autoscaler object for memory utilization].

[id="ocp-4-7-nodes-non-preempting-priority-classes"]
==== Non-preempting option for priority classes (Technology Preview)

You can now configure a priority class to be non-preempting by setting the `preemptionPolicy` field to `Never`. Pods with this priority class setting are placed in the scheduling queue ahead of lower priority pods, but do not preempt other pods.

For more information, see xref:../nodes/pods/nodes-pods-priority.html#non-preempting-priority-class_nodes-pods-priority[Non-preempting priority classes].

[id="ocp-4-7-logging"]
=== Cluster logging


[id="ocp-4-7-monitoring"]
=== Monitoring


[id="ocp-4-7-scale"]
=== Scale

[id="ocp-4-7-cnf-latency-test"]
==== Test to determine CPU latency
The latency test, a part of the CNF-test container, provides a way to measure if the isolated CPU latency is below the requested upper bound.

For information about running a latency test, see xref:../scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.adoc#cnf-performing-end-to-end-tests-running-the-latency_tests[Running the latency tests].

[id="ocp-4-7-dev-exp"]
=== Developer experience



[id="ocp-4-7-insights-operator"]
=== Insights Operator

[id="ocp-4-7-insights-operator-data-collection-enhancements"]
==== Insights Operator data collection enhancements

In {product-title} 4.7, the Insights Operator collects the following additional information:

* The top 100 `InstallPlan` entries to identify invalid Operator Lifecycle Manager (OLM) installations
* The service accounts from the Kubernetes default namespace and the `openshift*` built-in namespaces
* The `ContainerRuntimeConfig` and `MachineConfigPools` configuration files to verify container storage limits
* The configuration files for all available `operator.openshift.io` control pane resources to identify Operators in unmanaged states
* The `NetNamespaces` names, including their `netID` and egress IP addresses
* The `StatefulSet` configuration definitions from the {product-title} default namespaces to check the Prometheus monitoring storage type
* Appearances of certain log entries of pods in the `openshift-apiserver-operator` namespace
* Appearances of certain log entries of `sdn` pods in the `openshift-sdn` namespace

With this additional information, Red Hat can provide improved remediation steps in {cloud-redhat-com}.


[id="ocp-4-7-notable-technical-changes"]
== Notable technical changes

{product-title} 4.7 introduces the following notable technical changes.



[id="ocp-4-7-deprecated-removed-features"]
== Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within {product-title} {product-version}, refer to the table below. Additional details for more fine-grained functionality that has been deprecated and removed are listed after the table.

In the table, features are marked with the following statuses:

* *GA*: _General Availability_
* *DEP*: _Deprecated_
* *REM*: _Removed_

.Deprecated and removed features tracker
[cols="3,1,1,1",options="header"]
|====
|Feature |OCP 4.5 |OCP 4.6 |OCP 4.7

|`OperatorSource` objects
|DEP
|REM
|REM

|Package Manifest Format (Operator Framework)
|DEP
|DEP
|DEP

|`oc adm catalog build`
|DEP
|DEP
|DEP

|`--filter-by-os` flag for `oc adm catalog mirror`
|GA
|GA
|DEP

|v1beta1 CRDs
|DEP
|DEP
|DEP

|Docker Registry v1 API
|GA
|DEP
|

|Metering Operator
|GA
|DEP
|DEP

|====

[id="ocp-4-7-deprecated-features"]
=== Deprecated features

[id="ocp-4-7-filterbyos-deprecated"]
==== Catalog mirroring using filter-by-os flag

When using the `oc adm catalog mirror` command to mirror catalogs, the `--filter-by-os` flag was previously allowed to filter architectures of mirrored content. This would break references to those images in the catalog that point to the manifest list and not the manifest. The `--filter-by-os` flag now only filters the index image that is pulled and unpacked. To clarify this, the new `--index-filter-by-os` flag is now added and should be used instead.

The `--filter-by-os` flag is also now deprecated.

[id="ocp-4-7-removed-features"]
=== Removed features

[id="ocp-4-7-removed-provisioningHostIP-and-bootstrapProvisioningIP"]
==== Installer-provisioned clusters no longer require provisioningHostIP or bootstrapProvisioningIP

When using installer-provisioned installation on bare metal nodes, {product-title} 4.6 required providing two IP addresses from the `baremetal` network to the `provisioningHostIP` and `bootstrapProvisioningIP` configuration settings when deploying without a `provisioning` network. These IP addresses and configuration settings are no longer required in {product-title} 4.7 when using installer provisioned infrastructure on bare metal nodes and deploying without a `provisioning` network.


[id="ocp-4-7-bug-fixes"]
== Bug fixes



[id="ocp-4-7-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

In the table below, features are marked with the following statuses:

* *TP*: _Technology Preview_
* *GA*: _General Availability_
* *-*: _Not Available_

.Technology Preview tracker
[cols="4,1,1,1",options="header"]
|====
|Feature |OCP 4.5 |OCP 4.6 |OCP 4.7

|Precision Time Protocol (PTP)
|TP
|TP
|

|`oc` CLI Plug-ins
|TP
|TP
|

|experimental-qos-reserved
|TP
|TP
|

|Ephemeral Storage Limit/Requests
|TP
|TP
|

|Descheduler
|TP
|TP
|GA

|Podman
|TP
|TP
|

|OVN-Kubernetes Pod network provider
|TP
|GA
|GA

|HPA custom metrics adapter based on Prometheus
|TP
|TP
|

|HPA for memory utilization
|TP
|TP
|GA

|Service Binding
|TP
|TP
|

|Log forwarding
|TP
|GA
|GA

|Monitoring for user-defined projects
|TP
|GA
|GA

|Raw Block with Cinder
|TP
|TP
|TP

|External provisioner for AWS EFS
|TP
|TP
|TP

|CSI volume snapshots
|TP
|TP
|GA

|CSI volume cloning
|TP
|GA
|GA

|CSI volume expansion
|TP
|TP
|TP

|vSphere Problem Detector Operator
|-
|-
|GA

|CSI GCP PD Driver Operator
|-
|-
|TP

|CSI OpenStack Cinder Driver Operator
|-
|-
|TP

|CSI AWS EBS Driver Operator
|TP
|TP
|TP

|Red Hat Virtualization (oVirt) CSI Driver Operator
|-
|GA
|GA

|CSI inline ephemeral volumes
|TP
|TP
|TP

|Automatic device discovery and provisioning with Local Storage Operator
|-
|TP
|TP

|OpenShift Pipelines
|TP
|TP
|

|Vertical Pod Autoscaler
|TP
|TP
|

|Operator API
|TP
|GA
|GA

|Adding kernel modules to nodes
|TP
|TP
|

|====

[id="ocp-4-7-known-issues"]
== Known issues

// TODO: This known issue should carry forward to 4.7 and beyond!
* In {product-title} 4.1, anonymous users could access discovery endpoints. Later releases revoked this access to reduce the possible attack surface for security exploits because some discovery endpoints are forwarded to aggregated API servers. However, unauthenticated access is preserved in upgraded clusters so that existing use cases are not broken.
+
If you are a cluster administrator for a cluster that has been upgraded from {product-title} 4.1 to 4.7, you can either revoke or continue to allow unauthenticated access. It is recommended to revoke unauthenticated access unless there is a specific need for it. If you do continue to allow unauthenticated access, be aware of the increased risks.
+
[WARNING]
====
If you have applications that rely on unauthenticated access, they might receive HTTP `403` errors if you revoke unauthenticated access.
====
+
Use the following script to revoke unauthenticated access to discovery endpoints:
+
[source,bash]
----
## Snippet to remove unauthenticated group from all the cluster role bindings
$ for clusterrolebinding in cluster-status-binding discovery system:basic-user system:discovery system:openshift:discovery ;
do
### Find the index of unauthenticated group in list of subjects
index=$(oc get clusterrolebinding ${clusterrolebinding} -o json | jq 'select(.subjects!=null) | .subjects | map(.name=="system:unauthenticated") | index(true)');
### Remove the element at index from subjects array
oc patch clusterrolebinding ${clusterrolebinding} --type=json --patch "[{'op': 'remove','path': '/subjects/$index'}]";
done
----
+
This script removes unauthenticated subjects from the following cluster role bindings:
+
--
** `cluster-status-binding`
** `discovery`
** `system:basic-user`
** `system:discovery`
** `system:openshift:discovery`
--
+
(link:https://bugzilla.redhat.com/show_bug.cgi?id=1821771[*BZ#1821771*])

[id="ocp-4-7-asynchronous-errata-updates"]
== Asynchronous errata updates

Security, bug fix, and enhancement updates for {product-title} 4.7 are released as asynchronous errata through the Red Hat Network. All {product-title} 4.7 errata is https://access.redhat.com/downloads/content/290/[available on the Red Hat Customer Portal]. See the https://access.redhat.com/support/policy/updates/openshift[{product-title} Life Cycle] for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified via email whenever new errata relevant to their registered systems are released.

[NOTE]
====
Red Hat Customer Portal user accounts must have systems registered and consuming {product-title} entitlements for {product-title} errata notification emails to generate.
====

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of {product-title} 4.7. Versioned asynchronous releases, for example with the form {product-title} 4.7.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.

[IMPORTANT]
====
For any {product-title} release, always review the instructions on xref:../updating/updating-cluster.adoc#updating-cluster[updating your cluster] properly.
====

[id="ocp-4-7-0-ga"]
=== RHBA-2021:1234 - {product-title} 4.7 image release and bug fix advisory

Issued: 2021-xx-xx

{product-title} release 4.7 is now available. The list of bug fixes that are included in the update is documented in the link:https://access.redhat.com/errata/RHBA-2020:1234[RHBA-2020:1234] advisory. The RPM packages that are included in the update are provided by the link:https://access.redhat.com/errata/RHBA-2020:5678[RHBA-2020:5678] advisory.

Space precluded documenting all of the container images for this release in the advisory. See the following article for notes on the container images in this release:

link:https://access.redhat.com/solutions/<ARTICLE_ID>[{product-title} 4.7.0 container image list]
